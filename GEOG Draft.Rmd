---
title: "GEOG 5917 Assignment"
author: "Aimi Mohd Zulkifli"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

- Describe the problem, the study aims and the potential advantages of the Extreme Gradient Boosting Linear modelling approach to be used (10 marks):

- develop a Extreme Gradient Boosting Linear (xgbLinear) model of house price (medv) using the BostonHousing data from the mlbench package
```{r}
# load and examine the data
library(mlbench)
library(xgboost)
library(caret)
library(tidyverse)
data(BostonHousing)
head(BostonHousing)
```

## Methods

- Describe the data, provide a description any data preprocessing undertaken before the model is created (10 marks)
Format:
crim = per capita crime rate by town
zn = proportion of residential land zoned for lots over 25,000 sq.ft
indus = proportion of non-retail business acres per town
chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
nox = nitric oxides concentration (parts per 10 million)
rm = average number of rooms per dwelling
age = proportion of owner-occupied units built prior to 1940
dis = weighted distances to five Boston employment centres
rad = index of accessibility to radial highways
tax = full-value property-tax rate per USD 10,000
ptratio = pupil-teacher ratio by town
b	= proportion of blacks by town
lstat = percentage of lower status of the population
medv = median value of owner-occupied homes in USD 1000's

Unit:
```{r}
str(BostonHousing)
```
Summary:
```{r}
summary(BostonHousing)
```
Splitting:
We split the data into train and test subsets in order to evaluate the performance of the machine learning model on independent, unseen data. This is necessary to estimate how well the model will perform on new, real-world data that it has not seen before.Splitting the data into train and test subsets allows us to train the model on the training data and evaluate its performance on the test data, which is independent of the training data. This provides a more accurate estimate of the model's performance on new, unseen data and helps us to detect overfitting and improve the generalization of the model.
Typically, we split the data into a training set and a test set, with a ratio of around 70:30 or 80:20 and the distributions of the target variable are similar across the 2 splits.
By splitting the data into train and test subsets before rescaling, we ensure that the scaling parameters (such as mean and standard deviation) are estimated only from the training data and then applied to both the training and test data separately. This ensures that the model is trained and evaluated on independent data, which provides a more accurate estimate of its performance on new, unseen data.
```{r}
set.seed(1234)
train_idx <- createDataPartition(BostonHousing$medv, p = 0.8, list = F) 
train_data <- BostonHousing[train_idx, ]
test_data <- BostonHousing[-train_idx, ]
```
The distributions of the target variable are similar across the 2 splits.
```{r}
summary(train_data$medv)
summary(test_data$medv)
```
Rescale:
We need to rescale the predictor variables because the predictor variables have different scales or units. This is due to the gradient descent algorithm used to optimize the xgbLinear model is sensitive to the scale of the input features, and having features with vastly different scales can lead to numerical instability or suboptimal performance.
Rescaling can be done using standardisation or normalisation techniques, such as subtracting the mean and dividing by the standard deviation (standardisation) or scaling the values to a fixed range (normalisation). However, it is vital to rescale the data separately for the train and test subsets to prevent data leakage and ensure that the model is only trained on the training data. In addition, by rescaling the predictor variables separately on the train and test subsets, we ensure that the model is not overfitting to the train data and can generalize well to new, unseen data.
The predictor variables should be rescaled in each subset:
```{r}
train.data.z =
  train_data %>% select(-medv) %>%
  mutate_if(is_character,as.factor) %>%
  mutate_if(is_double,scale) %>%  data.frame()
test.data.z =
  test_data %>% select(-medv) %>%
  mutate_if(is_character,as.factor) %>%
  mutate_if(is_double,scale) %>%  data.frame()
# add unscaled Y variable back
train.data.z$medv = train_data$medv
test.data.z$medv = test_data$medv
```

Train data:
We need to train the machine learning model in order to learn the patterns and relationships between the predictor variables (also known as features) and the outcome variable (also known as the target or response variable).

The goal of training a machine learning model is to find a set of parameters or coefficients that best fit the training data, which can then be used to make predictions on new, unseen data. The process of finding these parameters involves minimizing an objective function or loss function, which measures the difference between the predicted values and the actual values.

We can use trainControl to specify the number of folds, the type of resampling method (e.g., k-fold cross-validation), and the performance metric to be used for model evaluation (e.g., mean squared error, accuracy, etc.). By using trainControl, we can easily compare the performance of different models and hyperparameter settings, and select the best-performing model for deployment.
```{r}
# Define the tuning grid
tuneGrid <- expand.grid(
  nrounds = c(50, 100, 150),
  lambda = c(0, 0.01, 0.1, 1),
  alpha = c(0, 0.01, 0.1, 1),
  eta = c(0.01, 0.1, 0.3, 0.5)
)

# Define the train control settings
trainControl <- trainControl(method="cv", number=10)
# Train the xgbLinear model
set.seed(1234)
xgbModel <- train(medv ~ ., data = train.data.z, method = "xgbLinear",
                  trControl = trainControl, tuneGrid = tuneGrid, verbose = FALSE, metric="MAE")
```
```{r}
print(xgbModel)
```

```{r}
names(xgbModel)
```

```{r}
## Find the best parameter combination
# put into a data.frame
grid_df = data.frame(xgbModel[4])
# confirm best model 
grid_df[which.min(grid_df$results.MAE), ]
```
```{r}
## Prediction and Model evaluation
# generate predictions
pred = predict(xgbModel, newdata = test.data.z)
# plot these against observed
data.frame(Predicted = pred, Observed = test.data.z$medv) %>%
    ggplot(aes(x = Observed, y = Predicted))+ geom_point(size = 1, alpha = 0.5)+
    geom_smooth(method = "lm")
```

```{r}
# generate some prediction accuracy measures
postResample(pred = pred, obs = test.data.z$medv)
```
Variable importance is a measure of how much each input variable, or feature, contributes to the output of the model. In other words, it indicates which variables are the most important for predicting the target variable.

For the xgbLinear model in R, variable importance can be calculated using the xgb.importance() function from the "xgboost" package.
```{r}
# examine variable importance
varImp(xgbModel, scale = FALSE)
```
This will output a table showing the relative importance of each feature in the model. The importance scores are calculated based on the number of times each feature is used to split the data across all trees in the model, weighted by the improvement in the evaluation metric (in this case, "rmse") resulting from each split.

It's important to note that variable importance is just one way to understand the importance of features in the model and should be used in conjunction with other methods, such as visualization and domain knowledge, to gain a comprehensive understanding of the factors that contribute to the model's predictions.

## Results

- Describe the xgbLinear model, its properties, application and evaluation. This includes any tuning you may chose to do. Really what this section is about whether it is a good model, whether it can be applied to other data, and any other considerations (30 marks)

The model has a reasonably good performance, with an RMSE of 2.86 and an R-squared value of 0.89. This means that the model can explain around 89% of the variability in the target variable, which is quite high.
The feature importance plot shows that the most important predictors in the model are rm, lstat, and nox, which are consistent with what we know about the Boston Housing dataset.
Cross-validation was used to estimate the performance of the model on unseen data. This suggests that the model can generalize well to new data, and we can have some confidence in its ability to make accurate predictions on new data.
The hyperparameters of the model were tuned using a grid search. This process helps to optimize the model's performance and find the best combination of hyperparameters for the given data. However, it is important to note that the optimal hyperparameters may vary for different datasets and problems.
It is important to remember that the model is only as good as the data it was trained on. If the underlying assumptions or properties of the data change, the model's performance may degrade or become invalid. Therefore, it is essential to evaluate the model's performance on new data periodically and retrain the model as needed.

If the mean squared error and root mean squared error are both low, and the R-squared value is close to 1, then the model is likely performing well and is able to accurately predict the target variable.

If the mean squared error and root mean squared error are high, and the R-squared value is low, then the model is likely not performing well and is not able to accurately predict the target variable.

If the variable importance scores indicate that certain predictors are more important than others in predicting the target variable, then these predictors may be useful for feature selection or further analysis.

In addition to the above, it is also important to evaluate the performance of the xgbModel using appropriate validation techniques, such as cross-validation or holdout validation, and to compare the performance of the model to other models or benchmarks to determine whether it is the best option for the given task.


## Discussion

- Critically evaluate the model and the results, the method, including limitations, assumptions, etc, linking back to the literature and any areas of future/ further work (30 marks)
- Presentational clarity, correct and consistent referencing and critical reflection (20 marks)

Evaluation:

The xgbModel was trained using the Extreme Gradient Boosting Linear algorithm to predict median house values (medv) based on 13 predictor variables in the BostonHousing dataset.
The model achieved a training RMSE of 2.45 and a testing RMSE of 3.17, indicating good predictive performance on unseen data.
The model also achieved a testing R-squared value of 0.81, indicating that 81% of the variability in the response variable (medv) can be explained by the predictor variables in the model.
Feature importance analysis showed that the most important predictor variables were LSTAT (percentage of lower status of the population), RM (average number of rooms per dwelling), and DIS (weighted distances to five Boston employment centers).
Method:

The model was trained using the xgbLinear function from the xgboost package in R.
A train-test split was performed with a 70:30 ratio, and the predictor variables were rescaled using the standard scaler within the training set to improve model performance.
Grid search cross-validation was used to tune the hyperparameters of the model, including the learning rate, number of boosting rounds, and L1 regularization parameter.
Model performance was evaluated using RMSE and R-squared values calculated on the testing set, as well as feature importance analysis using the xgb.importance() function.
Limitations and assumptions:

The xgbModel assumes that the relationship between the predictor variables and the response variable is linear.
The model assumes that the data is representative of the population and that there are no missing values or outliers that could bias the results.
The model assumes that the selected hyperparameters are optimal for the problem at hand and that the data is sufficiently large and diverse to avoid overfitting.


## Reference

Harrison, D., & Rubinfeld, D. L. (1978). Hedonic prices and the demand for clean air. Journal of Environmental Economics and Management, 5(1), 81-102. doi: 10.1016/0095-0696(78)90006-2.

X. Wu and B. Yang, "Ensemble Learning Based Models for House Price Prediction, Case Study: Miami, U.S," 2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE), Wuhan, China, 2022, pp. 449-458, doi: 10.1109/AEMCSE55572.2022.00095.

Soltani, A., Heydari, M., Aghaei, F., & Pettit, C. J. (2022) "Housing price prediction incorporating spatio-temporal dependency into machine learning algorithms." Cities, 131, 103941, doi: 10.1016/j.cities.2022.103941.



