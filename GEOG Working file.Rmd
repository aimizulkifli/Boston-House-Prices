---
title: "GEOG5917 Assignment"
author: "Aimi"
date: "2023-03-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 2: creating a xgbLinear model using the iris data

This has 5 variables and the the model is constructed to predict the Sepal.Length (target variable)
```{r}
## load data and packages
library(xgboost)
library(caret)
library(tidyverse)
data(iris)
head(iris)
```
The `createDataPartition` function can be used to create training and testing (validation) subsets
```{r}
set.seed(1234) # for reproducibility
train.index = createDataPartition(iris$Sepal.Length, p = 0.8, list = F) 
data.train = iris[train.index,]
data.test = iris[-train.index,]
```
The distributions of the target variable are similar across the 2 splits:
```{r}
summary(data.train$Sepal.Length)
summary(data.test$Sepal.Length)
```
The predictor variables should be rescaled in each subset:
```{r}
data.train.z =
  data.train %>% select(-Sepal.Length) %>%
  mutate_if(is_character,as.factor) %>%
  mutate_if(is_double,scale) %>%  data.frame()
data.test.z =
  data.test %>% select(-Sepal.Length) %>%
  mutate_if(is_character,as.factor) %>%
  mutate_if(is_double,scale) %>%  data.frame()
# add unscaled Y variable back
data.train.z$Sepal.Length = data.train$Sepal.Length
data.test.z$Sepal.Length = data.test$Sepal.Length
```
And the `trainControl()` function used to define the type of ‘inmodel’ sampling and evaluation undertaken to iteratively refine the model. It generates a list of parameters that are passed to the train function that creates the model. Here a simple 10 fold cross validation will suffice:
```{r}
trainControl <- trainControl(method="cv", number=10)
```
Then the model can be run over the grid, setting a seed for reproducibility:
```{r}
## run the model over the grid
set.seed(99)
m.caret <- train(Sepal.Length ~ ., data=data.train.z,  method="xgbLinear",
              trControl=trainControl, verbose=FALSE, metric="MAE")
```
And the result examined:
```{r}
## Examine the results
print(m.caret)
# explore the results 
names(m.caret)
# see best tune 
m.caret[6]
```
The best parameter combinations can be determined:
```{r}
## Find the best parameter combination
# put into a data.frame
grid_df = data.frame(m.caret[4])
# confirm best model 
grid_df[which.min(grid_df$results.MAE), ]
```
The final model can be evaluated, in this case using held back validation / testing data, and predictions evaluated against observations
```{r}
## Prediction and Model evaluation
# generate predictions
pred = predict(m.caret, newdata = data.test.z)
# plot these against observed
data.frame(Predicted = pred, Observed = data.test.z$Sepal.Length) %>%
    ggplot(aes(x = Observed, y = Predicted))+ geom_point(size = 1, alpha = 0.5)+
    geom_smooth(method = "lm")
```
```{r}
# generate some prediction accuracy measures
postResample(pred = pred, obs = data.test.z$Sepal.Length)
```
```{r}
# examine variable importance
varImp(m.caret, scale = FALSE)
```


